{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "142aed34",
   "metadata": {},
   "source": [
    "# Segment Anything Model 2 (SAM 2) by Andrés Felipe Forero Mendez\n",
    "\n",
    "## ¿Qué es SAM 2?\n",
    "\n",
    "**Segment Anything Model 2 (SAM 2)** es un modelo avanzado de visión por computadora desarrollado por Meta AI, diseñado para realizar **segmentación de imágenes**. La **segmentación de imágenes** es un proceso clave que permite dividir una imagen en regiones distintas, donde cada región corresponde a un objeto o parte de un objeto específico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db2acc2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://agencia-ia.com/wp-content/uploads/2023/04/Que-es-SAM-la-ia-para-segmentar-imagenes-de-facebook.jpg\"  width=\"500\" height=\"400\"/>\n",
    "    <p> <span style=\"font-weight: bold;\">Imagen 1.</span> Ejemplo de como SAM 2 segmenta las imagenes. Fuente: <a href=\"https://agencia-ia.com/que-es-sam-la-revolucion-en-segmentacion-de-imagenes-mediante-ia/\">URL</a></p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b9aa6",
   "metadata": {},
   "source": [
    "\n",
    "### ¿Cómo Funciona?\n",
    "\n",
    "El SAM 2 está basado en arquitecturas de **transformers** que son capaces de aprender relaciones complejas entre las diferentes partes de una imagen. Además, SAM 2 tiene una capacidad única de permitir la interacción del usuario, donde se puede seleccionar manualmente áreas de interés, proporcionando más control y precisión en la segmentación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b902ef",
   "metadata": {},
   "source": [
    "<h4>Arquitectura transformers:</h4> \n",
    "<p>La arquitectura de transformers es un tipo de modelo de inteligencia artificial diseñado para procesar secuencias de datos (como texto o imágenes) de manera más eficiente. Utiliza un mecanismo llamado atención, que le permite enfocarse en diferentes partes de la secuencia al mismo tiempo, en lugar de procesarla en orden como lo hacen otros modelos. (Redes Neuronales Recurrentes)\n",
    "\n",
    "Los transformers son especialmente buenos para tareas de lenguaje, como traducción, generación de texto, y procesamiento de imágenes, ya que pueden capturar relaciones a largo plazo entre palabras o elementos en la secuencia. Además, son rápidos porque pueden trabajar con todos los elementos simultáneamente, en lugar de procesarlos uno por uno. Esto los hace muy potentes en aplicaciones como ChatGPT, BERT y otros grandes modelos de lenguaje.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497292e1",
   "metadata": {},
   "source": [
    "## Impacto en la industria\n",
    "\n",
    "***Automatización y Robótica:*** Nos permite a los robots identificar y segmentar objetos en tiempo real, facilitando tareas de manipulación, ensamblaje, y navegación autónoma en entornos industriales. \n",
    "\n",
    "***Visión por Computadora:*** En sistemas de control de calidad, puede segmentar piezas defectuosas en líneas de producción, lo que ayuda a detectar problemas antes de que se conviertan en fallos costosos.\n",
    "\n",
    "***Sistemas de Mantenimiento Predictivo:*** Las cámaras de monitoreo pueden segmentar componentes específicos de maquinaria en movimiento, permitiendo un análisis detallado y la detección de desgastes o anomalías, mejorando el mantenimiento preventivo.\n",
    "\n",
    "***Agricultura de Precisión:*** En campos como la agricultura, que combinan tecnologías de IoT y robótica, nos puede segmentar cultivos, identificar áreas afectadas por plagas o analizar el estado de crecimiento, mejorando el rendimiento de la producción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600fb2ea",
   "metadata": {},
   "source": [
    "## Ventajas de SAM 2\n",
    "\n",
    "1. **Segmentación Universal**: No necesita entrenamientos específicos para cada tipo de objeto.\n",
    "\n",
    "2. **Capacidad de Interacción**: El usuario puede indicar áreas de interés, mejorando la precisión de la segmentación.\n",
    "\n",
    "3. **Segmentación en Tiempo Real**: Procesa imágenes de forma eficiente, lo que lo hace útil en aplicaciones de alta demanda de tiempo, como la robótica o la monitorización de sistemas.\n",
    "\n",
    "4. **Aplicabilidad Amplia**: Se puede usar en múltiples dominios, como imágenes médicas, satelitales, agrícolas, etc.\n",
    "\n",
    "## Desventajas de SAM 2\n",
    "\n",
    "1. **Requiere Gran Cantidad de Recursos Computacionales**: Al ser un modelo grande, demanda un alto uso de GPU para lograr eficiencia en tiempo real.\n",
    "\n",
    "2. **Posible Falta de Precisión en Imágenes Desafiantes**: En algunos casos, como imágenes de muy baja resolución o con objetos muy pequeños, SAM 2 puede no ser tan preciso.\n",
    "\n",
    "3. **Dependencia de Interacción del Usuario**: Aunque la interacción mejora la precisión, esto puede no ser ideal para todos los casos, especialmente donde la automatización completa es crítica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4a0aed",
   "metadata": {},
   "source": [
    "\n",
    "## ¿En qué está realizado SAM 2?\n",
    "\n",
    "SAM 2 está desarrollado principalmente en **Python**, utilizando el framework de aprendizaje profundo **PyTorch**. Las bibliotecas de apoyo incluyen **OpenCV** para procesamiento de imágenes y **NumPy** para manipulación de datos. \n",
    "\n",
    "### Componentes clave en el desarrollo de SAM 2:\n",
    "\n",
    "***PyTorch:*** Framework de aprendizaje profundo que maneja las operaciones de entrenamiento del modelo, backpropagation y optimización.\n",
    "\n",
    "***NumPy y OpenCV:*** Se usan estas bibliotecas para procesamiento y manipulación de imágenes antes de enviarlas al modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a499886d",
   "metadata": {},
   "source": [
    "\n",
    "## Usos de SAM 2 y Ejemplos\n",
    "\n",
    "1. **Medicina**: Segmentación de órganos y tejidos en imágenes médicas.\n",
    "2. **Agricultura de Precisión**: Monitoreo de cultivos y detección de plagas a través de imágenes obtenidas por drones.\n",
    "3. **Robótica**: Identificación de objetos y caminos en entornos complejos para la navegación autónoma.\n",
    "4. **Edición de Imágenes y Videos**: Facilita la manipulación de contenido visual al permitir segmentaciones precisas de objetos.\n",
    "\n",
    "    ***Tesla*** utiliza robots equipados con sistemas de visión avanzados para identificar y ensamblar piezas en sus líneas de producción de automóviles, reduciendo errores humanos y aumentando la eficiencia.\n",
    "\n",
    "    ***Siemens*** emplea visión por computadora en sus fábricas para realizar mantenimiento predictivo, reduciendo el tiempo de inactividad y optimizando la vida útil de sus equipos.\n",
    "\n",
    "    ***John Deere*** ha desarrollado maquinaria agrícola que utiliza IA para segmentar y analizar campos de cultivo, permitiendo la siembra y recolección de manera eficiente, optimizando recursos.\n",
    "\n",
    "    ***Tesla Autopilot***, utiliza una combinación de redes neuronales profundas (DNNs) y la arquitectura de transformers para procesar y analizar datos de su entorno en tiempo real.\n",
    "\n",
    "**Redes Neuronales Convolucionales (CNNs):** Se utilizan principalmente para el procesamiento de imágenes de las cámaras instaladas en el vehículo. Estas redes analizan las imágenes y detectan objetos como vehículos, peatones, señales de tráfico, etc.\n",
    "\n",
    "**Arquitectura de Transformers:** Se ha integrado para mejorar la interpretación de secuencias de datos y relaciones espaciales, lo que ayuda a comprender mejor la dinámica de la carretera y las interacciones entre los objetos detectados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a926e6c",
   "metadata": {},
   "source": [
    "## Retos Técnicos: \n",
    "\n",
    "1. ***Precision en ambientes complejos***\n",
    "\n",
    "2. ***Adaptabilidad***\n",
    "\n",
    "## Consideraciones Éticas:\n",
    "\n",
    "1. ***Desplazamiento Laboral***\n",
    "\n",
    "2. ***Responsabilidad en decisiones automatizadas***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eed79e",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusión\n",
    "\n",
    "El **Segment Anything Model 2** de Meta AI representa una evolución significativa en el campo de la visión por computadora. Su capacidad de segmentar objetos de cualquier tipo sin necesidad de entrenamientos específicos, junto con la posibilidad de interactuar con el usuario para mejorar la segmentación, lo convierte en una herramienta poderosa para una amplia gama de industrias.\n",
    "\n",
    "\n",
    "La IA ***SAM2*** tiene el potencial de cambiar radicalmente la ingeniería mecatrónica. Puede hacer que los procesos sean más eficientes, anticipar fallos antes de que ocurran, y ayudar a diseñar sistemas más efectivos. Además, mejora la automatización y facilita la integración de dispositivos conectados. Al analizar grandes cantidades de datos, realizar simulaciones avanzadas y optimizar la interacción entre humanos y máquinas, SAM2 promete transformar cómo se diseñan, operan y mantienen los sistemas mecatrónicos, haciendo que sean más eficientes y adaptables en la industria.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f505c4",
   "metadata": {},
   "source": [
    "\n",
    "A continuación, presentamos un ejemplo de código que utiliza SAM 2 y una prueba de cuanto poder de CPU se necesita para solicitar un codigo:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6339d65",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Librerias:</h3>\n",
    "\n",
    "<p>pip install torch</p>\n",
    "<p>pip install transformers</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c79cec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andres Mendez\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\Andres Mendez\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Andres Mendez\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9869323372840881}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andres Mendez\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Cargar el pipeline de clasificación de texto con un modelo preentrenado\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "# Texto para clasificar\n",
    "text = \"I love using transformers models in Python!\"\n",
    "\n",
    "# Realizar la clasificación\n",
    "result = classifier(text)\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aa2248",
   "metadata": {},
   "source": [
    "<h1>Practica: </h1>\n",
    "\n",
    "\n",
    "URL: https://colab.research.google.com/drive/1SGsVg491G7_B32lAKyouAr91MGKJ0ZKe?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
